# Security Review

**File**: `./app/services/ai_service.py`  
**Time**: 02:03:42  
**Type**: security_review

## Improvement

Okay, here's one significant security improvement you can make to the code, along with a secured version and explanation:

**The Improvement: Input Sanitization to Prevent Prompt Injection**

The code directly incorporates user input (`question`) into the prompt sent to the AI model. This is a classic vulnerability to prompt injection attacks.  A malicious user could craft a question designed to manipulate the AI's behavior, bypass intended restrictions, or even extract sensitive information.

**Secured Code:**

```python
import logging
from google import genai
from app.utils.config import get_settings
from app.services.product_data_service import ProductDataService
import re
import bleach  # Import the bleach library

# Setup logging
logger = logging.getLogger(__name__)

class AIService:
    def __init__(self):
        """Initialize AI service with Google AI API"""
        try:
            settings = get_settings()
            # Use the new Google AI client
            self.client = genai.Client(api_key=settings.GOOGLE_API_KEY)
            self.product_service = ProductDataService()
            logger.info("Successfully initialized AI service with Google AI client")
        except Exception as e:
            logger.error(f"Error initializing AI service: {str(e)}")
            raise

    async def get_response(self, question: str) -> str:
        """Get AI response with product context and fallback message"""
        try:
            logger.info(f"Getting AI response for question: {question}")

            # Sanitize the user input
            sanitized_question = bleach.clean(question, strip=True)

            # Ekstrak kategori dan max_price dari pertanyaan (sederhana)
            category = None
            max_price = None
            
            # Deteksi kategori dengan lebih lengkap (sama dengan API endpoint)
            question_lower = sanitized_question.lower()  # Use the sanitized version
            category_mapping = {
                'laptop': ['laptop', 'notebook', 'komputer'],
                'smartphone': ['smartphone', 'hp', 'handphone', 'phone', 'telepon', 'ponsel'],
                'tablet': ['tablet', 'ipad'],
                'headphone': ['headphone', 'earphone', 'headset', 'audio'],
                'kamera': ['kamera', 'camera', 'fotografi'],
                'audio': ['audio', 'speaker', 'sound'],
                'tv': ['tv', 'televisi'],
                'drone': ['drone', 'quadcopter'],
                'jam': ['jam', 'watch', 'smartwatch']
            }
            
            for cat, keywords in category_mapping.items():
                if any(keyword in question_lower for keyword in keywords):
                    category = cat
                    break
            
            # ... (rest of your code using sanitized_question)

            # Example of prompt construction (modify as needed, but SANITIZE any user input!)
            prompt = f"User question: {sanitized_question}.  Based on available product data for category '{category}', answer the user's question concisely."

            # Replace with your actual AI call using the prompt. This is a placeholder
            response = self.client.generate_text(prompt) # This is a placeholder

            logger.info(f"AI response: {response}")
            return response

        except Exception as e:
            logger.error(f"Error getting AI response: {str(e)}")
            return "I'm sorry, I encountered an error while processing your request."

```

**Explanation:**

1.  **`bleach` Library:** I've added `import bleach`. `bleach` is a Python library specifically designed for sanitizing HTML and text.  It's more robust than simple string replacement because it's designed to handle various HTML encoding and injection techniques.  Install it with `pip install bleach`.

2.  **Sanitization:**  The key line is:
    ```python
    sanitized_question = bleach.clean(question, strip=True)
    ```
    This line cleans the user's `question` using `bleach.clean()`. The `strip=True` argument removes leading and trailing whitespace. By default, bleach escapes HTML tags.  You can customize bleach's behavior with allowed tags and attributes if you need to allow some formatting, but for general protection against prompt injection, the default settings are a good starting point.

3.  **Using the Sanitized Input:** The code now uses `sanitized_question.lower()` for category extraction.  Crucially, `sanitized_question` is also used when constructing the prompt that is sent to the AI model.

4.  **Prompt Construction Example:** The prompt construction `prompt = f"User question: {sanitized_question}..."` is *just an example*.  The *most important* thing is that **any** user input that goes into the prompt must be sanitized. How you structure your prompts for the AI is up to you, but make sure all user-provided strings are cleaned.

**Why This Helps:**

*   **Prevents HTML Injection:**  `bleach` removes or escapes potentially harmful HTML tags that could be used to alter the AI's interpretation of the prompt.
*   **Reduces Attack Surface:**  By sanitizing the input, you significantly reduce the ways a malicious user can influence the AI's behavior.

**Important Considerations:**

*   **Context-Aware Sanitization:**  The best sanitization strategy depends on the specific AI model and how you're using it.  You might need to experiment with different `bleach` configurations (allowed tags, attributes, etc.) to find the right balance between security and functionality.
*   **Defense in Depth:** Sanitization is only one layer of defense.  Consider other security measures, such as:
    *   **Rate Limiting:**  Prevent users from sending too many requests in a short period of time.
    *   **Content Filtering:**  Implement filters to block prompts that contain offensive or malicious keywords.
    *   **AI Model Security:**  Use AI models that have built-in security features and are regularly updated.
*   **Ongoing Monitoring:**  Monitor your AI service for suspicious activity and adapt your security measures as needed.
*   **Prompt Engineering:** Design prompts that are less susceptible to manipulation. For example, use clear instructions and delimiters to separate user input from the rest of the prompt.
*   **Regularly Review and Update:**  Prompt injection techniques are constantly evolving, so it's essential to stay up-to-date on the latest threats and update your security measures accordingly.

This input sanitization is a crucial step in securing your AI service against prompt injection attacks. Remember to test your sanitization thoroughly and adapt it as needed to meet your specific requirements.

---
*Generated by Smart AI Bot*
