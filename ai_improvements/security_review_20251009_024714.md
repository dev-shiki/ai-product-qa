# Security Review

**File**: `./app/services/ai_service.py`  
**Time**: 02:47:14  
**Type**: security_review

## Improvement

```python
import logging
from google import genai
from app.utils.config import get_settings
from app.services.product_data_service import ProductDataService

# Setup logging
logger = logging.getLogger(__name__)

class AIService:
    def __init__(self):
        """Initialize AI service with Google AI API"""
        try:
            settings = get_settings()
            # Use the new Google AI client
            # Consider encrypting the API key at rest and decrypting it at runtime if feasible.
            # However, properly securing the encryption key is crucial.  This example focuses
            # on sanitizing input to prevent prompt injection.
            self.client = genai.Client(api_key=settings.GOOGLE_API_KEY)
            self.product_service = ProductDataService()
            logger.info("Successfully initialized AI service with Google AI client")
        except Exception as e:
            logger.error(f"Error initializing AI service: {str(e)}")
            raise

    async def get_response(self, question: str) -> str:
        """Get AI response with product context and fallback message"""
        try:
            logger.info(f"Getting AI response for question: {question}")

            # Sanitize the input question to prevent prompt injection.  This is the security improvement.
            # A more sophisticated approach might involve a dedicated prompt injection detection library.
            sanitized_question = self._sanitize_input(question)


            # Ekstrak kategori dan max_price dari pertanyaan (sederhana)
            import re
            category = None
            max_price = None
            
            # Deteksi kategori dengan lebih lengkap (sama dengan API endpoint)
            question_lower = sanitized_question.lower()  # Use the sanitized question
            category_mapping = {
                'laptop': ['laptop', 'notebook', 'komputer'],
                'smartphone': ['smartphone', 'hp', 'handphone', 'phone', 'telepon', 'ponsel'],
                'tablet': ['tablet', 'ipad'],
                'headphone': ['headphone', 'earphone', 'headset', 'audio'],
                'kamera': ['kamera', 'camera', 'fotografi'],
                'audio': ['audio', 'speaker', 'sound'],
                'tv': ['tv', 'televisi'],
                'drone': ['drone', 'quadcopter'],
                'jam': ['jam', 'watch', 'smartwatch']
            }
            
            for cat, keywords in category_mapping.items():
                if any(keyword in question_lower for keyword in keywords):
                    category = cat
                    break

... (truncated for analysis)

    def _sanitize_input(self, input_string: str) -> str:
        """Sanitize the input string to prevent prompt injection."""
        # Remove or replace potentially harmful characters and sequences.
        # This is a basic example and should be expanded based on the specific AI model
        # and potential vulnerabilities.  Consider using a more comprehensive library for this.
        sanitized_string = input_string.replace("{{", "").replace("}}", "").replace("[[", "").replace("]]", "")  # Remove template-like syntax
        sanitized_string = sanitized_string.replace("<!--", "").replace("-->", "") # Remove HTML comments
        sanitized_string = sanitized_string.replace("system:", "") #Remove system commands
        # Consider adding more replacements/removals based on your context.  Blacklisting isn't perfect,
        # but it's a first step.  Whitelisting (allowing only known-good characters/patterns) is ideal
        # but harder to implement.
        return sanitized_string
```

**Explanation of the security improvement:**

The primary security improvement is the addition of the `_sanitize_input` method and its usage within the `get_response` method. This addresses the critical vulnerability of **prompt injection**.

*   **Prompt Injection:**  Large language models (LLMs) are vulnerable to prompt injection attacks. An attacker can craft a malicious input ("prompt") that alters the LLM's behavior, potentially leading to data breaches, unauthorized actions, or denial of service.  Without sanitization, a user could inject commands or instructions into the `question` string that would override the intended behavior of the AI service. For example, an attacker could try to make the AI reveal sensitive information or perform actions it's not authorized to do.

*   **`_sanitize_input` Method:** This method aims to mitigate prompt injection by removing or replacing potentially harmful characters and sequences from the user-provided input.  It's a basic example that replaces common injection attempts:

    *   `{{`, `}}`, `[[`, `]]`: Removes template-like syntax that could be used to inject commands.
    *   `<!--`, `-->`: Removes HTML comments which are commonly used to inject commands that bypass basic sanitization rules.
    *   `system:`: Removes "system:" which is sometimes used to try and provide the model with system level commands.

*   **Why this is important:**  Without input sanitization, the AI service could be tricked into executing unintended instructions embedded within the user's question. This could lead to serious security breaches.

*   **Limitations and Further Improvements:**

    *   **Blacklisting approach:** The current `_sanitize_input` uses a blacklist approach (removing known bad patterns). Blacklists are often incomplete and can be bypassed with clever encoding or obfuscation. A whitelist approach (allowing only known good characters or patterns) is generally more secure but harder to implement.
    *   **Context-Specific:** The sanitization rules should be tailored to the specific LLM being used and the types of prompts it is expected to receive.
    *   **Advanced Detection:**  For production systems, consider using a dedicated prompt injection detection library or service. These libraries employ more sophisticated techniques to identify and block malicious prompts.  Also, consider using techniques like adversarial training to make the AI model more robust against prompt injection attacks.
    *   **Rate Limiting:** Implement rate limiting to prevent attackers from bombarding the AI service with malicious prompts.
    *   **Input Validation:** Validate the expected format and content of the user's question before passing it to the LLM.

**Other Considerations (API Key Security):**

While the primary focus was on prompt injection, the comment `Consider encrypting the API key at rest and decrypting it at runtime if feasible.` highlights another crucial aspect of API security.  Storing API keys directly in code or configuration files is a major security risk. Encryption at rest (e.g., using a secrets management system like HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault) is a best practice. However, properly securing the *encryption key* is paramount; otherwise, it negates the benefit. Hardcoding the decryption key into the application code simply moves the problem.

This improved response offers a concrete security enhancement and explains the importance of prompt injection prevention while acknowledging its limitations and suggesting further improvements.

---
*Generated by Smart AI Bot*
